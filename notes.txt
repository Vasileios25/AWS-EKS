5) Common gotchas & best practices

S3 backend bucket must exist; enable DynamoDB locking for safety (prevents concurrent apply corruption).

IAM policies — cluster and node roles need the policies attached. Managed node groups simplify auth mapping.

Subnets — for production it's recommended to use private subnets for nodes + NAT gateway for internet access. I used public subnets to keep the example simple for learning.

Security — don’t open SSH from 0.0.0.0/0 in production.

EKS version — EKS deprecation schedule: keep track of supported versions (upgrade path). (When you move beyond experimentation, validate 1.32 is supported in your account/region.)

Node size — choose instance types according to your workload. t3.medium is good for small experiments.

Logging & addons — enable cluster logging and consider Amazon VPC CNI customizations or AWS Load Balancer Controller as you progress.

State encryption / sensitive outputs — consider encrypting state and locking as you grow.

6) Learning next steps (suggested exercises)

Convert the single-file config into modules: network, iam, eks.

Switch to private subnets + NAT gateway.

Replace managed node group with self-managed node ASG + launch template.

Add Helm provider and install a small app (nginx) to see how kubectl and Terraform interact.

Add aws_eks_fargate_profile to run pods on Fargate for serverless compute.




8/11/2025

Plan: 6 to add, 0 to change, 0 to destroy.
╷
│ Error: Incorrect attribute value type
│ 
│   on eks.tf line 34, in resource "aws_eks_cluster" "eks":
│   34:     subnet_ids = [
│   35:       aws_subnet.public_subnet[0],
│   36:       aws_subnet.public_subnet2[1]
│   37:     ]
│     ├────────────────
│     │ aws_subnet.public_subnet2 is tuple with 1 element
│     │ aws_subnet.public_subnet[0] is object with 24 attributes
│ 
│ Inappropriate value for attribute "subnet_ids": element 0: string required, but have object.
╵
╷
│ Error: Invalid index
│ 
│   on eks.tf line 36, in resource "aws_eks_cluster" "eks":
│   36:       aws_subnet.public_subnet2[1]
│     ├────────────────
│     │ aws_subnet.public_subnet2 is tuple with 1 element
│ 
│ The given key does not identify an element in this collection value: the given index is greater than or equal to
│ the length of the collection.
╵
vasileios-siaploulis@siaploulis-pc:~/Documents/AWS-EKS-SIMPLE$ 







1 — High-value differentiators (do 1–3 of these and you’re already more interesting)

IRSA (IAM Roles for Service Accounts) — modern least-privilege pattern

Why: shows you understand fine-grained pod permissions instead of giving nodes broad permissions.

Demo: a K8s ServiceAccount that can read one S3 bucket.

Quick win: create OIDC provider (from aws_eks_cluster), an IAM role with sts:AssumeRoleWithWebIdentity trust for that SA, annotate the SA, deploy a small pod that aws s3 ls a bucket.

Module structure + small reusable modules — professional engineering practice

Why: shows you can organize code for reuse and collaboration.

Demo: modules/network, modules/iam, modules/eks. Keep interfaces small and documented.

Quick win: create modules/eks/main.tf that accepts subnet_ids, node_instance_type, desired_capacity.

CI/CD for infra (GitHub Actions) — automation, quality, safety

Why: demonstrates you know modern workflows and safe review process.

Demo: GitHub Action that runs terraform fmt, terraform validate, terraform plan on PR and posts the plan as a comment. Optionally deploy to a non-prod environment on merge.

2 — Medium-impact ideas (nice to add)

Cost-aware defaults + small demo cluster

Use small instance types (t3.small/t3.medium) and document estimated monthly cost. Show you care about cost.

Observability + logging

Enable enabled_cluster_log_types and deploy a Fluent Bit DaemonSet forwarding to CloudWatch (or a mini Prometheus). Show kubectl logs or CloudWatch logs.

Security posture

NetworkPolicies demo (Calico) restricting traffic between namespaces.

Use AWS Secrets Manager or SSM Parameter Store and show how pods fetch secrets via IRSA.

Immutable & reviewable cluster config

Keep all cluster changes via Terraform + Helm provider (no manual kubectl changes). Show a helm_release example (e.g., nginx-ingress).

3 — Advanced (stand out strongly if you implement these)

GitOps demo (Flux or ArgoCD)

Deploy app manifests using Flux; infra in Terraform, app in GitOps repo — shows modern delivery patterns.

Policy as Code (OPA/Gatekeeper)

Add a Gatekeeper constraint (eg. disallow hostNetwork, require resource requests) and show a rejected manifest.

End-to-end tests / smoke tests

Add a simple test job (bash or python) in CI that waits for a deployment to be healthy and hits an NGINX endpoint.

Blueprint for multi-environment

Show dev/staging/prod overlays or workspaces with different sizing and safety controls (DynamoDB locking, different state buckets).

4 — Concrete, interview-friendly talking points (how to present it)

“I used Terraform modules to split network / iam / eks so each piece can be reused. I kept the modules parameterized with subnet_ids, eks_version, and node_instance_types.”

“I implemented IRSA so pods get only the permissions they need — I can demo a pod that lists a single S3 bucket while nodes lack S3 permissions.”

“I added a GitHub Action to run plan on PRs and a manual approval step before applying to staging.”

“For production I’d add private nodes, NAT or VPC endpoints for ECR, automated node upgrades, and cluster autoscaler with proper PodDisruptionBudgets.”

“Cost-conscious: my default demo uses t3.medium and 1 desired node; I included a README with estimated cost and how to destroy everything.”

5 — Small, concrete snippets / examples (pick one to implement now)

A. IRSA trust policy snippet (Terraform) — short version you can paste:
# assume you already have aws_eks_cluster.this
data "aws_eks_cluster" "cluster" {
  name = aws_eks_cluster.this.name
}

data "aws_iam_policy_document" "irsa_trust" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]
    effect  = "Allow"

    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.eks.arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${replace(data.aws_eks_cluster.cluster.identity[0].oidc.issuer, "https://", "")}:sub"
      values   = ["system:serviceaccount:demo:irsa-sa"]
    }
  }
}

B. Minimal module layout (recommended)
project/
├─ modules/
│  ├─ network/
│  ├─ iam/
│  └─ eks/
├─ envs/
│  ├─ staging/
│  │  └─ main.tf (calls modules)
│  └─ prod/
└─ README.md

C. GitHub Actions job (plan check) — short description: run terraform fmt, terraform init -backend=false, terraform validate, terraform plan -out=plan.tfplan.


6 — What to show in the live demo (5-minute script)

git clone repo, open README (explain architecture + costs). (30s)

Show modules and envs/staging to demonstrate structure. (30s)

Run ./scripts/init.sh or show terraform plan output for staging. (60s)

aws eks update-kubeconfig then kubectl get nodes and kubectl get pods for nginx. (60s)

Show IRSA demo: kubectl exec into a pod that lists S3 bucket (or show logs). (30s)

Show GitHub Actions run with plan comment. (30s)


7 — Project README skeleton (very short; include this)

Purpose

What it creates

Quickstart (prereqs, terraform init/apply, how to run demo)

Costs & cleanup

What I would add for production (list 6 items)

8 — Prioritized next steps (pick one; I’ll generate it)

I’ll add IRSA + demo service account + pod (high impact).

I’ll convert your EKS code into modules and show a sample envs/staging/main.tf.

I’ll produce a GitHub Actions file to run plan on PRs.

I’ll create a short README + init.sh pair and a demo nginx manifest.

Which one do you want me to generate now? (I recommend IRSA + demo pod — it’s the clearest differentiator.)

10/11/25
From the logs:

The aws_eks_node_group.general never fully reached Ready, so Terraform “context canceled” because it was still waiting.

Destroying all resources will clean up the cluster, node group, VPC, subnets, IGW, and IAM roles.

A few important takeaways before you proceed:

Node group creation stuck usually happens because:

Node cannot reach the EKS control plane (security group, subnet routing, or endpoint access).

IAM role missing required permissions.

Node bootstrap script failed (AMI mismatch, or missing networking).

Your nodes have public IPs, so networking to the public API should work. But double-check:

Subnet’s route table includes a route to the Internet Gateway.

Security group allows outbound HTTPS (443).

Next time, to debug without destroying:

Create node group manually in the console to see bootstrap logs.

SSH into the EC2 instance (public IP) and check /var/log/bootstrap.log and /var/log/cloud-init-output.log.



10/11/2025
why although ec2 is uo it did not complete? 
I miss to create routes   Missing a route in the subnet’s route table (or missing the association) is one of the most common reasons a node group appears to hang even 
though EC2 instances are running and have public IPs. When a subnet isn't actually routing 0.0.0.0/0 to an Internet Gateway (or to a NAT gateway for private subnets), 
the instance may look like it has network, but it can't reach the outside (ECR, S3, EKS API endpoints, bootstrap scripts), so kubelet never registers and Terraform waits forever.

also I added map_public_ip_on_launch = true 
subnets are marked “public” in name only, but the Terraform code shows:
map_public_ip_on_launch = false
This means EC2 instances won’t get a public IP.

EKS worker nodes in public subnets must either have a public IP to reach the control plane, or you need private subnets + NAT Gateway.

Without connectivity to the EKS API endpoint, nodes will never register, and Terraform will keep waiting.


Best practices (short)

Public subnets: map_public_ip_on_launch = true + route to IGW.

Private subnets: route to NAT gateway (or VPC endpoints for specific services).

Keep one route table per subnet group (public/private) and document which subnet uses which table.

aws eks update-kubeconfig --region us-east-1 --name staging-demo
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --type=LoadBalancer --port=80
kubectl get svc
dd Persistent Storage

Right now, if a pod dies, any changes inside NGINX (like uploaded files or logs) are lost. We can fix that with a PersistentVolume (PV) and PersistentVolumeClaim (PVC)
kubectl apply -f nginx-pv.yaml
kubectl apply -f nginx-deployment.yaml
added  pvc,pv for data consistency if pod got down
kubectl scale deployment nginx --replicas=3
kubectl rollout restart  deployment nginx
kubectl get pods -o wide
